# -*- coding: utf-8 -*-
"""narrativeqa_bart_finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nK4X9oKw1Vu5hDPNIdYOzItJML0m_2hh
"""

import json
from multiprocessing import Pool
from tqdm import tqdm
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from datasets import load_dataset, load_metric
from transformers import AutoTokenizer
from transformers.models.bart.modeling_bart import shift_tokens_right

import os

os.environ["WANDB_DISABLED"] = "true"


class NarrativeQADataset(Dataset):
    def __init__(self,
                 split="train",
                 use_multiprocessing: bool = True,
                 multiprocessing_chunksize: int = -1,
                 process_count: int = 2):
        """
        NarrativeQADataset: Creates a Pytorch Dataset for the NarrativeQA abstractiveQA
        :param split: train, validation, test split of data
        :param use_multiprocessing: whether to use multiprocessing or not
        :param multiprocessing_chunksize: size of a chunk to be passed to a multiprocessor
        :param process count: number of multiprocessor
        """
        self.narrative_qa_dataset = load_dataset("narrativeqa")[split]
        self.use_multiprocessing = use_multiprocessing
        self.multiprocessing_chunksize = multiprocessing_chunksize
        self.process_count = process_count
        print("self.use_multiprocessing", self.use_multiprocessing)
        self.make_samples()

    def preprocess_sample(self, sample):
        """
        Each sample of the preprocessed dataset comprises of a dictionary of format
                                            {"question": str,
                                             "document": str,
                                             "target": str}
        Each sample of NarrativeQA data comprises of a question and its associated list of answers. We split each answer into multiple samples by repeating the question with its unique answer.
        :param sample: A NarrativeQA sample
        :return: A NarrativeQA sample
        """
        question = "<question> " + sample["question"]["text"].strip()
        document = "<title> " + sample["document"]["summary"]["title"].strip().replace("\n", " ") + " <context> " + \
                   sample["document"]["summary"]["text"].strip().replace("\n", " ")
        preprocessed = []
        for i, answer in enumerate(sample["answers"]):
            preprocessed.append({  # "source": question + " " + document,
                "question": question,
                "document": document,
                "target": answer["text"].strip().replace("\n", " ")}
            )
        return preprocessed

    def make_samples(self):
        """Creates the NarrativeDataset list of samples"""
        if self.use_multiprocessing:
            print("make_samples use_multiprocessing")
            if self.multiprocessing_chunksize == -1:
                chunksize = max(len(self.narrative_qa_dataset) // (self.process_count * 2), 500)
            else:
                chunksize = self.multiprocessing_chunksize

            with Pool(self.process_count) as p:
                samples = list(
                    tqdm(p.imap(self.preprocess_sample, self.narrative_qa_dataset, chunksize=chunksize),
                         total=len(self.narrative_qa_dataset),
                         disable=False,
                         )
                )
                self.samples = []
                for samp in samples:
                    self.samples += samp
        else:
            print("make_samples without use_multiprocessing")
            self.samples = []
            for indx, sample in enumerate(tqdm(self.narrative_qa_dataset, disable=False)):
                # if indx > 10:
                #    break
                preprocessed = self.preprocess_sample(sample)
                self.samples += preprocessed
            print('Processing Samples complete')

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        return self.samples[index]


class NarrativeQAProcessor():
    def __init__(self,
                 training_dataset: Dataset = None,
                 eval_dataset: Dataset = None,
                 tokenizer: AutoTokenizer = None,
                 decoder_start_token_id=0,
                 decoder_max_length: int = 100,
                 test_dataset: Dataset = None,
                 is_test: bool = False,
                 **params):
        """
        Generated tokenized batches for training, evaluation and testing of seq2seq task
        :param training_dataset: A NarrativeQA of training samples
        :param eval_dataset: A NarrativeQA of evaluation samples
        :param docder_max_length:
        :param tokenizer: Tokenizer for tokenizing the samples of NarrativeQA dataset
        :param test_dataset: Optional NarrativeQA of test samples

        """

        self.decoder_max_length = 512
        self.decoder_start_token_id = decoder_start_token_id
        # tokenizer_class = params["tokenizer"] if "tokenizer" in params.keys() else "bert-base-uncased"
        self.tokenizer = tokenizer

        if not is_test:
            self.training_dataset = training_dataset
            self.eval_dataset = eval_dataset
            self.sampler = RandomSampler(data_source=self.training_dataset)
            self.training_generator = DataLoader(self.training_dataset,
                                                 sampler=self.sampler,
                                                 collate_fn=self.collate,
                                                 **params)
            self.eval_generator = DataLoader(self.eval_dataset,
                                             sampler=SequentialSampler(data_source=self.eval_dataset),
                                             collate_fn=self.collate,
                                             **params)
        if is_test:
            self.test_dataset = test_dataset
            self.test_generator = DataLoader(self.test_dataset,
                                             sampler=SequentialSampler(data_source=self.test_dataset),
                                             collate_fn=self.collate_generate,
                                             **params)

    def shift_tokens_right(self, input_ids, pad_token_id):
        """Shift input ids one token to the right, and wrap the last non pad token (usually <eos>)."""
        prev_output_tokens = input_ids.clone()
        index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
        prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()
        prev_output_tokens[:, 1:] = input_ids[:, :-1]
        return prev_output_tokens

    def collate(self, batch):
        """
        Generates tokenized batches
        """
        tokenized_input = self.tokenizer.prepare_seq2seq_batch(batch["source"], batch["target"], max_length=512,
                                                               max_target_length=self.decoder_max_length,
                                                               truncation=True, padding='max_length',
                                                               return_tensors="pt")

        decoder_input_ids = shift_tokens_right(tokenized_input['labels'], self.tokenizer.pad_token_id)

        return {"input_ids": tokenized_input["input_ids"],
                "attention_mask": tokenized_input["attention_mask"],
                "labels": tokenized_input["labels"],
                "decoder_input_ids": decoder_input_ids}  # tokenized_input["labels"]}#decoder_input_ids["input_ids"]}

    def collate_generate(self, batch):
        """
        Generates tokenized batches
        """
        source = [samp["question"] + " " + samp["document"] for samp in batch]
        target = [samp["target"] for samp in batch]
        tokenized_input = self.tokenizer.prepare_seq2seq_batch(source, target, max_length=512,
                                                               max_target_length=100,
                                                               truncation=True, padding='max_length',
                                                               return_tensors="pt")

        decoder_input_ids = shift_tokens_right(tokenized_input['labels'], self.tokenizer.pad_token_id,
                                               self.decoder_start_token_id)

        return {"input_ids": tokenized_input["input_ids"],
                "attention_mask": tokenized_input["attention_mask"],
                "labels": tokenized_input["labels"],
                "decoder_input_ids": decoder_input_ids}